[
  {
    "objectID": "posts/Blog5/index.html",
    "href": "posts/Blog5/index.html",
    "title": "Blog-4: Uncover Anomalies in England’s Weather: A Data-driven Exploration",
    "section": "",
    "text": "Uncover Anomalies in England’s Weather: A Data-driven Exploration\nIntroduction\n#In this blog post, we take a deep dive into England’s weather records using Python and libraries like pandas, seaborn, and matplotlib. Our goal is to uncover anomalies in temperature, wind, pressure, and humidity through statistical analysis and machine learning. ```\nExploring the Dataset\n\nimport pandas as pd\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.express as px\nimport datetime as dt\nfrom datetime import timedelta\nimport matplotlib.dates as mdates\nplt.rcParams['figure.figsize'] = (12,6)\nplt.style.use('fivethirtyeight')\n\n\ndf=pd.read_csv(\"EnglandWeather.csv\")\ndf\ndf.head().style.set_properties(**{'background-color':'lightgreen','color':'black','border-color':'#8b8c8c'})\ndf.shape\ndf.dtypes\ndf.sort_values('Formatted Date', inplace= True)\ndf\n\n\n\n\n\n\n\n\nFormatted Date\nSummary\nPrecip Type\nTemperature (C)\nWind Speed (km/h)\nPressure (millibars)\nHumidity\n\n\n\n\n2880\n2006-01-01 00:00:00.000 +0100\nPartly Cloudy\nrain\n0.577778\n17.1143\n1016.66\n0.89\n\n\n2881\n2006-01-01 01:00:00.000 +0100\nMostly Cloudy\nrain\n1.161111\n16.6152\n1016.15\n0.85\n\n\n2882\n2006-01-01 02:00:00.000 +0100\nMostly Cloudy\nrain\n1.666667\n20.2538\n1015.87\n0.82\n\n\n2883\n2006-01-01 03:00:00.000 +0100\nOvercast\nrain\n1.711111\n14.4900\n1015.56\n0.82\n\n\n2884\n2006-01-01 04:00:00.000 +0100\nMostly Cloudy\nrain\n1.183333\n13.9426\n1014.98\n0.86\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n89728\n2016-12-31 19:00:00.000 +0100\nMostly Cloudy\nrain\n0.488889\n9.7566\n1020.03\n0.86\n\n\n89729\n2016-12-31 20:00:00.000 +0100\nMostly Cloudy\nrain\n0.072222\n9.4185\n1020.27\n0.88\n\n\n89730\n2016-12-31 21:00:00.000 +0100\nMostly Cloudy\nsnow\n-0.233333\n9.2736\n1020.50\n0.89\n\n\n89731\n2016-12-31 22:00:00.000 +0100\nMostly Cloudy\nsnow\n-0.472222\n9.2414\n1020.65\n0.91\n\n\n89732\n2016-12-31 23:00:00.000 +0100\nMostly Cloudy\nsnow\n-0.677778\n9.2253\n1020.72\n0.92\n\n\n\n\n96453 rows × 7 columns\n\n\n\nData Preprocessing:\n\n#We clean and preprocess the data by converting the 'Formatted Date' column to a datetime object and extracting additional temporal features.\n\n\ndf['Formatted Date'] = pd.to_datetime(df.sort_values('Formatted Date')['Formatted Date'],utc=True)\ndf['Formatted Date'] = df['Formatted Date'].sort_values()\ndf['Formatted Date']\ndf.dtypes\ndf.isnull().sum()\ndf['Day'] = df['Formatted Date'].dt.day\ndf['Month'] = df['Formatted Date'].dt.month\ndf['Year'] = df['Formatted Date'].dt.year\ndf.groupby('Year')\nYear = pd.to_datetime(df['Formatted Date']).dt.year\nMonth = pd.to_datetime(df['Formatted Date']).dt.month\n\nDetecting Anomalies using Z-Scores:\n\n#A simple way to detect potential anomalies is by calculating z-scores. Z-scores measure how many standard deviations each data point is from the mean. We compute z-scores and Euclidean distances for key weather variables like temperature, wind, pressure, and humidity. Points with high Euclidean distances are flagged as potential anomalies.\n\nvariables = ['Temperature (C)', 'Wind Speed (km/h)', 'Pressure (millibars)', 'Humidity']\nz_scores = (df[variables] - df[variables].mean()) / df[variables].std()\nz_scores\neuclidean_distance = pd.DataFrame({'euclidean_distance': z_scores.apply(lambda x: x**2).sum(axis=1)**0.5})\nthreshold = euclidean_distance.mean() + 3 * euclidean_distance.std()\nthreshold\n\neuclidean_distance    4.914545\ndtype: float64\n\n\n\n#Identify the anomalies as those data points with a score above the threshold.\n\n\nanomalies = df[euclidean_distance &gt; threshold]\nsns.set_style(\"darkgrid\")\nsns.scatterplot(data=df, x='Temperature (C)', y='Pressure (millibars)')\nsns.scatterplot(data=anomalies, x='Temperature (C)', y='Pressure (millibars)', color='red')\n\n&lt;Axes: xlabel='Temperature (C)', ylabel='Pressure (millibars)'&gt;\n\n\n\n\n\nIsolation Forest\n\n# We employ the Isolation Forest algorithm, which isolates anomalies by randomly splitting the data.After training a model, we can score each data point based on how anomalous the model predicts it to be.\n\n#Looking at the humidity vs temperature scatterplot colored by these anomaly scores clearly highlights the most unusual points. Isolation Forests excel at detecting anomalies that are hard to find with traditional statistical methods.\n\n\nfrom sklearn.ensemble import IsolationForest\nfeatures = ['Temperature (C)', 'Wind Speed (km/h)', 'Pressure (millibars)', 'Humidity']\nX = df[features]\nmodel = IsolationForest(contamination=0.05)\nmodel.fit(X)\ndf['anomaly_score'] = model.decision_function(X)\ndf['anomaly'] = model.predict(X)\nsns.scatterplot(data=df, x='Temperature (C)', y='Humidity', hue='anomaly', palette='colorblind')\n\n&lt;Axes: xlabel='Temperature (C)', ylabel='Humidity'&gt;\n\n\n\n\n\nConclusion\n\n#By combining statistical analysis and machine learning, we've uncovered intriguing anomalies in England's weather data. Understanding these irregularities helps ensure data quality and prompts deeper meteorological insights. As the volume of weather data grows, having data-driven anomaly detection tools becomes increasingly important for researchers and forecasters."
  },
  {
    "objectID": "posts/Blog3/index.html",
    "href": "posts/Blog3/index.html",
    "title": "Blog-5: Detecting Credit Card Fraud: A Comprehensive Analysis Using SVM",
    "section": "",
    "text": "Title: Detecting Credit Card Fraud: A Comprehensive Analysis Using SVM\nIntroduction\n#In this blog post, we delve into the world of credit card transactions, utilizing machine learning techniques, specifically Support Vector Machines (SVM), to detect fraudulent activities. Using the Python programming language and various libraries such as pandas, numpy, scikit-learn, and seaborn, we aim to build a robust fraud detection model and evaluate its performance on a real-world credit card dataset. ```\nExploring the Dataset\n\nimport pandas as pd \nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import svm\nimport itertools\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab\nimport seaborn\n%matplotlib inline\n\n\ndata=pd.read_csv(\"creditcard.csv\")\n\nVisualizing Fraudulent Transactions:\n\n#Here we understand the distribution of fraudulent transactions is crucial. We visualize fraud amounts over time to identify potential patterns.\n\n\ndata.head()\ndf = pd.DataFrame(data) \ndf.describe()\ndf_fraud = df[df['Class'] == 1] # Recovery of fraud data\nplt.figure(figsize=(15,10))\nplt.scatter(df_fraud['Time'], df_fraud['Amount']) # Display fraud amounts according to their time\nplt.title('Scratter plot amount fraud')\nplt.xlabel('Time')\nplt.ylabel('Amount')\nplt.xlim([0,175000])\nplt.ylim([0,2500])\nplt.show()\n\n\n\n\n\nnb_big_fraud = df_fraud[df_fraud['Amount'] &gt; 1000].shape[0] # Recovery of frauds over 1000\nprint('There are only '+ str(nb_big_fraud) + ' frauds where the amount was bigger than 1000 over ' + str(df_fraud.shape[0]) + ' frauds')\n\nThere are only 9 frauds where the amount was bigger than 1000 over 492 frauds\n\n\n\nnumber_fraud = len(data[data.Class == 1])\nnumber_no_fraud = len(data[data.Class == 0])\nprint('There are only '+ str(number_fraud) + ' frauds in the original dataset, even though there are ' + str(number_no_fraud) +' no frauds in the dataset.')\nprint(\"The accuracy of the classifier then would be : \"+ str((284315-492)/284315)+ \" which is the number of good classification over the number of tuple to classify\")\n\nThere are only 492 frauds in the original dataset, even though there are 284315 no frauds in the dataset.\nThe accuracy of the classifier then would be : 0.998269524998681 which is the number of good classification over the number of tuple to classify\n\n\nExploring Correlation and Feature\n\n#To visualize the correlation heatmap and identify the most influential features.\n\ndf_corr = df.corr()\nplt.figure(figsize=(15,10))\nseaborn.heatmap(df_corr, cmap=\"YlGnBu\") # Displaying the Heatmap\nseaborn.set(font_scale=2,style='white')\n\nplt.title('Heatmap correlation')\nplt.show()\n\n\n\n\n\nrank = df_corr['Class'] \ndf_rank = pd.DataFrame(rank) \ndf_rank = np.abs(df_rank).sort_values(by='Class',ascending=False) \n                                                                  \ndf_rank.dropna(inplace=True) \n\n\ndf_train_all = df.iloc[:150000, :]  # Select the first 150,000 rows\ndf_train_1 = df_train_all[df_train_all['Class'] == 1]\ndf_train_0 = df_train_all[df_train_all['Class'] == 0]\n\nprint('In this dataset, we have ' + str(len(df_train_1)) + ' frauds, so we need to take a similar number of non-frauds')\n\ndf_sample = df_train_0.sample(300)\ndf_train = pd.concat([df_train_1, df_sample])  \ndf_train = df_train.sample(frac=1)  \n\nIn this dataset, we have 293 frauds, so we need to take a similar number of non-frauds\n\n\nBuilding and Testing the Model\n\n#we split the data into training and testing sets. The training data is used to build our SVM model, while the testing data lets us evaluate how well it identifies frauds.We feed the training features and labels into the SVM to teach it the patterns of legitimate and fraudulent transactions. SVMs work by finding the optimal boundaries between classes based on the input data. Once trained, we can predict labels for the testing set and see how well the model generalizes. We'll use custom metrics focused on fraud detection to rigorously assess its performance. If it falls short, we can iterate and tweak the model until it reliably flags the tricky frauds.\n\n\nX_train = df_train.drop(['Time', 'Class'],axis=1)\ny_train = df_train['Class'] \nX_train = np.asarray(X_train)\ny_train = np.asarray(y_train)\ndf_test_all = df[150000:]\nX_test_all = df_test_all.drop(['Time', 'Class'],axis=1)\ny_test_all = df_test_all['Class']\nX_test_all = np.asarray(X_test_all)\ny_test_all = np.asarray(y_test_all)\ndf_test_all = df[150000:]\nX_test_all = df_test_all.drop(['Time', 'Class'],axis=1)\ny_test_all = df_test_all['Class']\nX_test_all = np.asarray(X_test_all)\ny_test_all = np.asarray(y_test_all)\nX_train_rank = df_train[df_rank.index[1:11]] \nX_train_rank = np.asarray(X_train_rank)\nX_test_all_rank = df_test_all[df_rank.index[1:11]]\nX_test_all_rank = np.asarray(X_test_all_rank)\ny_test_all = np.asarray(y_test_all)\nclass_names=np.array(['0','1'])\n\nEvaluating the Model:\n\n#We evaluate the SVM classifier using confusion matrices and custom evaluation criteria.\n\n\ndef plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = 'd' \n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] &gt; thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n\nclassifier = svm.SVC(kernel='linear')\nclassifier.fit(X_train, y_train)\nprediction_SVM_all = classifier.predict(X_test_all)\ncm = confusion_matrix(y_test_all, prediction_SVM_all)\nplot_confusion_matrix(cm,class_names)\n\n\n\n\n\nprint('Our criterion give a result of ' \n      + str( ( (cm[0][0]+cm[1][1]) / (sum(cm[0]) + sum(cm[1])) + 4 * cm[1][1]/(cm[1][0]+cm[1][1])) / 5))\nprint('We have detected ' + str(cm[1][1]) + ' frauds / ' + str(cm[1][1]+cm[1][0]) + ' total frauds.')\nprint('\\nSo, the probability to detect a fraud is ' + str(cm[1][1]/(cm[1][1]+cm[1][0])))\nprint(\"the accuracy is : \"+str((cm[0][0]+cm[1][1]) / (sum(cm[0]) + sum(cm[1]))))\n\nOur criterion give a result of 0.9260122595515577\nWe have detected 184 frauds / 199 total frauds.\n\nSo, the probability to detect a fraud is 0.9246231155778895\nthe accuracy is : 0.9315688354462306\n\n\n\n#Rebalancing Weights\n\n\nclassifier_b = svm.SVC(kernel='linear',class_weight={0:0.60, 1:0.40})\nclassifier_b.fit(X_train, y_train)\n\nSVC(class_weight={0: 0.6, 1: 0.4}, kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(class_weight={0: 0.6, 1: 0.4}, kernel='linear')\n\n\n\nprediction_SVM_b_all = classifier_b.predict(X_test_all)\ncm = confusion_matrix(y_test_all, prediction_SVM_b_all)\nplot_confusion_matrix(cm,class_names)\n\n\n\n\n\nprint('Our criterion give a result of ' \n      + str( ( (cm[0][0]+cm[1][1]) / (sum(cm[0]) + sum(cm[1])) + 4 * cm[1][1]/(cm[1][0]+cm[1][1])) / 5))\nprint('We have detected ' + str(cm[1][1]) + ' frauds / ' + str(cm[1][1]+cm[1][0]) + ' total frauds.')\nprint('\\nSo, the probability to detect a fraud is ' + str(cm[1][1]/(cm[1][1]+cm[1][0])))\nprint(\"the accuracy is : \"+str((cm[0][0]+cm[1][1]) / (sum(cm[0]) + sum(cm[1]))))\n\nOur criterion give a result of 0.9162456074835891\nWe have detected 181 frauds / 199 total frauds.\n\nSo, the probability to detect a fraud is 0.9095477386934674\nthe accuracy is : 0.9430370826440764\n\n\nConclusion\n\n# In this post, we tackled credit card fraud detection using Support Vector Machines. We saw how imbalanced datasets, with far more normal transactions than frauds, can trip up standard models. By rebalancing class weights, we optimized our SVM to focus more on catching the frauds.SVMs provide a mighty tool for fraud fighting, but we can't just plug and play. Carefully evaluating metrics and tweaking for the data's nuances were key to making our model reliable in the real world.As we continue to advance in machine learning and data science, the fight against fraud remains an ever-evolving challenge with innovative solutions and techniques."
  },
  {
    "objectID": "posts/Blog1/index.html",
    "href": "posts/Blog1/index.html",
    "title": "Blog-1: Gaining Insights into Diabetes with Data Science",
    "section": "",
    "text": "Gaining Insights into Diabetes with Data Science\nIntroduction\n\n#Diabetes impacts millions globally. In this blog, we'll use Python to analyze a diabetes dataset, visualize key features, and build a predictive model with logistic regression. Join me on this journey into the data to uncover insights that could inform diabetes care and research.\n\nExploring the Dataset\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\ndf=pd.read_csv(\"diabetes.csv\")\ndf.head()\ndf.info()\ndf.describe(include='all')\nprint('Duplication')\ndf.duplicated().sum()\ndf.isnull().sum()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 768 entries, 0 to 767\nData columns (total 9 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Pregnancies               768 non-null    int64  \n 1   Glucose                   768 non-null    int64  \n 2   BloodPressure             768 non-null    int64  \n 3   SkinThickness             768 non-null    int64  \n 4   Insulin                   768 non-null    int64  \n 5   BMI                       768 non-null    float64\n 6   DiabetesPedigreeFunction  768 non-null    float64\n 7   Age                       768 non-null    int64  \n 8   Outcome                   768 non-null    int64  \ndtypes: float64(2), int64(7)\nmemory usage: 54.1 KB\nDuplication\n\n\nPregnancies                 0\nGlucose                     0\nBloodPressure               0\nSkinThickness               0\nInsulin                     0\nBMI                         0\nDiabetesPedigreeFunction    0\nAge                         0\nOutcome                     0\ndtype: int64\n\n\n\n#Exploratory Data Analysis\n\n\nplt.xlabel(\"BMI\")\nplt.ylabel('count')\nplt.figure(figsize=(6,6),facecolor='red',edgecolor='black',clear=False)\na=sns.countplot(x=df['Outcome'],data=df)\nfor ax in a.containers:\n    a.bar_label(ax)\n\n\n\n\n\n\n\n\n#Pregnencies count\n\n\nplt.xlabel(\"Pregnencies\")\nplt.ylabel('count')\nplt.figure(figsize=(6,6),facecolor='yellow',edgecolor='black',clear=False)\nb=sns.countplot(x=df['Pregnancies'],data=df)\nfor ax1 in b.containers:\n    b.bar_label(ax1)\n\n\n\n\n\n\n\n\n#from the observation we can tell that many women have only 1 pregnency   \n\nplt.xlabel(“Outcome”) plt.ylabel(‘Pregnency’) plt.figure(figsize=(6,6),facecolor=‘orange’,edgecolor=‘black’,clear=False) c=sns.barplot(x=df[‘Outcome’],y=df[‘Pregnancies’],data=df) for ax in c.containers: c.bar_label(ax)\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndf=pd.read_csv(\"diabetes.csv\")\ndf\nplt.figure(figsize=(10,10),facecolor='orange',edgecolor='Black',clear=True)\na=sns.heatmap(df.corr(),annot=True,fmt='.1g',linewidths=0.5)\na\n\n&lt;Axes: &gt;\n\n\n\n\n:::\n\nX=df.drop('Outcome',axis=1)\nY=df.Outcome\nX\nY\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2)\nX_train\nX_test\nY_train\nY_test\n\n162    0\n24     1\n745    0\n126    0\n347    0\n      ..\n161    0\n465    0\n149    0\n545    1\n298    1\nName: Outcome, Length: 154, dtype: int64\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nlr=LogisticRegression()\nlr.fit(X_train,Y_train)\nlr.predict(X_test)\nlr.predict([[1,200,100,20,200,28,0.3,30]])\n\nlr.score(X_test,Y_test)\nlr.predict_proba(X_test)\nlr.coef_\nlr.intercept_\n\narray([-8.20523468])\n\n\nConclusion\n\n#This exploration shows how data science techniques like visualization and modeling can unlock insights from diabetes data. There are many opportunities to further analyze this data and extract meaningful patterns to inform care and quality of life."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am Amal Sujith (ID: 906578048) student of MS in Computer Engineering. I’m currently working on Machine Learning and improving my problem solving skills.\nContact: Email: amalsmenon7@gmail.com Phone No: +918281725367"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to The World of Machine Learning (CS5805)",
    "section": "",
    "text": "Blog-1: Gaining Insights into Diabetes with Data Science\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nAmal Sujith\n\n\n\n\n\n\n  \n\n\n\n\nBlog-2: Analyzing Advertising Data with Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nAmal Sujith\n\n\n\n\n\n\n  \n\n\n\n\nBlog-3: Exploring Weather Patterns through Machine Learning\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nAmal Sujith\n\n\n\n\n\n\n  \n\n\n\n\nBlog-4: Uncover Anomalies in England’s Weather: A Data-driven Exploration\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nAmal Sujith\n\n\n\n\n\n\n  \n\n\n\n\nBlog-5: Detecting Credit Card Fraud: A Comprehensive Analysis Using SVM\n\n\n\n\n\n\n\n\n\n\n\n\nNov 9, 2023\n\n\nAmal Sujith\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Blog2/index.html",
    "href": "posts/Blog2/index.html",
    "title": "Blog-2: Analyzing Advertising Data with Linear Regression",
    "section": "",
    "text": "Analyzing Advertising Data with Linear Regression\nIntroduction\n\n#Advertising is key for sales, but quantifying its impact can be tricky. In this post, we'll use Python to explore an advertising dataset, visualize relationships, and build a model to predict sales based on ad spending.\n\nExploring the Dataset\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\ndf=pd.read_csv(\"Advertising.csv\")\ndf\n\n\n\n\n\n\n\n\nSmartphones\nRadio\nTV\nSales\n\n\n\n\n0\n230.1\n37.8\n69.2\n22.1\n\n\n1\n44.5\n39.3\n45.1\n10.4\n\n\n2\n17.2\n45.9\n69.3\n9.3\n\n\n3\n151.5\n41.3\n58.5\n18.5\n\n\n4\n180.8\n10.8\n58.4\n12.9\n\n\n...\n...\n...\n...\n...\n\n\n195\n38.2\n3.7\n13.8\n7.6\n\n\n196\n94.2\n4.9\n8.1\n9.7\n\n\n197\n177.0\n9.3\n6.4\n12.8\n\n\n198\n283.6\n42.0\n66.2\n25.5\n\n\n199\n232.1\n8.6\n8.7\n13.4\n\n\n\n\n200 rows × 4 columns\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 4 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Smartphones  200 non-null    float64\n 1   Radio        200 non-null    float64\n 2   TV           200 non-null    float64\n 3   Sales        200 non-null    float64\ndtypes: float64(4)\nmemory usage: 6.4 KB\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nSmartphones\nRadio\nTV\nSales\n\n\n\n\ncount\n200.000000\n200.000000\n200.000000\n200.000000\n\n\nmean\n147.042500\n23.264000\n30.554000\n14.022500\n\n\nstd\n85.854236\n14.846809\n21.778621\n5.217457\n\n\nmin\n0.700000\n0.000000\n0.300000\n1.600000\n\n\n25%\n74.375000\n9.975000\n12.750000\n10.375000\n\n\n50%\n149.750000\n22.900000\n25.750000\n12.900000\n\n\n75%\n218.825000\n36.525000\n45.100000\n17.400000\n\n\nmax\n296.400000\n49.600000\n114.000000\n27.000000\n\n\n\n\n\n\n\n\ndf.isnull().sum()\n\nSmartphones    0\nRadio          0\nTV             0\nSales          0\ndtype: int64\n\n\n\nplt.figure(figsize=(8, 10))\n\ncolumns_to_plot = ['Smartphones', 'Radio', 'TV', 'Sales']\n\nfor i, column in enumerate(columns_to_plot, 1):\n    plt.subplot(3, 2, i)\n    sns.histplot(data=df[column], kde=True)\n    plt.title(column)\n    plt.xlabel(column)\n\nplt.tight_layout()\nplt.show() \n\n\n\n\n\nsns.boxplot(data=df)\nplt.title('Box Plot')\nplt.show()\n\n\n\n\n\ncorrelation_matrix = df.corr()\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\nplt.title('Correlation Heatmap')\nplt.show()\n\n\n\n\n\nplt.scatter(df[\"Smartphones\"],df[\"Sales\"],alpha=0.5)\nplt.xlabel(\"Sales\")\nplt.ylabel(\"TV\")\nplt.title(\"Comparison of Sales with Smartphones\")\nplt.show()\n\n\n\n\n\nplt.scatter(df[\"Radio\"],df[\"Sales\"],alpha=0.5)\nplt.xlabel(\"Sales\")\nplt.ylabel(\"Radio\")\nplt.title(\"Comparison of Sales with Radio\")\nplt.show()\n\n\n\n\n\nplt.scatter(df[\"TV\"],df[\"Sales\"],alpha=0.5)\nplt.xlabel(\"Sales\")\nplt.ylabel(\"TV\")\nplt.title(\"Comparison of Sales with TV\")\nplt.show()\n\n\n\n\nLinear Regression Model\n\n#We build a linear regression model to predict sales based on the ad channels. After splitting the data into training and test sets, we fit and evaluate the model.It achieves decent performance in predicting sales numbers based on new ad input data. Checking the actuals vs predictions shows the relationship and where the model.\n\n\nX = df[['Smartphones', 'TV', 'Radio']]\ny = df['Sales']\nX.head()\ny.head()\n\n0    22.1\n1    10.4\n2     9.3\n3    18.5\n4    12.9\nName: Sales, dtype: float64\n\n\n\nX_train , X_test , y_train , y_test = train_test_split(X , y , test_size=0.2 , random_state=100)\nmodel = LinearRegression()\nmodel.fit(X_train , y_train )\ny_pred = model.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(\"Mean Squared Error:\", mse)\nprint(\"Mean Absolute Error:\", mae)\nprint(\"R-squared:\", r2)\n\nMean Squared Error: 1.7332927815807728\nMean Absolute Error: 1.0271679904924937\nR-squared: 0.9184369032278497\n\n\n\nplt.figure(figsize=(8, 6))\nplt.scatter(y_test, y_pred, alpha=0.5)\nplt.xlabel(\"Actual Values (y_test)\")\nplt.ylabel(\"Predicted Values (y_pred)\")\nplt.title(\"Actual vs. Predicted Values\")\nregression_line_x = np.linspace(min(y_test), max(y_test), 100)\nregression_line_y = regression_line_x\nplt.plot(regression_line_x, regression_line_y, color='red', linestyle='--', label=\"Regression Line\")\nplt.legend()\nplt.show()\n\n\n\n\nConclusion\n\n#While not perfect, the model uncovers clear links between advertising dollars and sales response. This enables data-driven decisions about marketing mix and budget allocation. There are many opportunities to improve the model with techniques like regularization and interactions. But it's a solid starting point to quantify advertising's impact."
  },
  {
    "objectID": "posts/Blog4/index.html",
    "href": "posts/Blog4/index.html",
    "title": "Blog-3: Exploring Weather Patterns through Machine Learning",
    "section": "",
    "text": "Exploring Weather Patterns through Machine Learning\nIntroduction\n\n#In this blog, we'll dive into granular weather data to uncover patterns using Python and K-Means clustering. Analyzing minute-by-minute observations can reveal valuable insights into weather conditions.\n\nExploring the Dataset\n\n#Loading and Preprocessing the Data\n\n#First, we load a dataset containing high-frequency weather data. It has measurements like temperature, humidity, wind speed, etc. taken every minute across many sensors. After sampling it down for feasibility, we do some prep like handling missing data.\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nimport pandas as pd\nimport numpy as np\nfrom itertools import cycle, islice\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import parallel_coordinates\n%matplotlib inline\n\n\ndata = pd.read_csv('minute_weather.csv')\ndata\ndata.shape\ndata.head()\nsampled_df = data[(data['rowID'] % 10) == 0]\nsampled_df.shape\nsampled_df.describe().transpose()\nsampled_df[sampled_df['rain_accumulation'] == 0].shape\nsampled_df[sampled_df['rain_duration'] == 0].shape\ndel sampled_df['rain_accumulation']\ndel sampled_df['rain_duration']\nrows_before = sampled_df.shape[0]\nsampled_df = sampled_df.dropna()\nrows_after = sampled_df.shape[0]\nrows_before - rows_after\nsampled_df.columns\n\nIndex(['rowID', 'hpwren_timestamp', 'air_pressure', 'air_temp',\n       'avg_wind_direction', 'avg_wind_speed', 'max_wind_direction',\n       'max_wind_speed', 'min_wind_direction', 'min_wind_speed',\n       'relative_humidity'],\n      dtype='object')\n\n\n\nfeatures = ['air_pressure', 'air_temp', 'avg_wind_direction', 'avg_wind_speed', 'max_wind_direction', \n        'max_wind_speed','relative_humidity']\n\nselect_df = sampled_df[features]\nselect_df.columns\nselect_df\nX = StandardScaler().fit_transform(select_df)\nX\n\narray([[-1.48456281,  0.24544455, -0.68385323, ..., -0.62153592,\n        -0.74440309,  0.49233835],\n       [-1.48456281,  0.03247142, -0.19055941, ...,  0.03826701,\n        -0.66171726, -0.34710804],\n       [-1.51733167,  0.12374562, -0.65236639, ..., -0.44847286,\n        -0.37231683,  0.40839371],\n       ...,\n       [-0.30488381,  1.15818654,  1.90856325, ...,  2.0393087 ,\n        -0.70306017,  0.01538018],\n       [-0.30488381,  1.12776181,  2.06599745, ..., -1.67073075,\n        -0.74440309, -0.04948614],\n       [-0.30488381,  1.09733708, -1.63895404, ..., -1.55174989,\n        -0.62037434, -0.05711747]])\n\n\nK-Means Clustering:\n\n#K-Means looks for distinct clusters within the data by minimizing differences within clusters and maximizing differences between them. We have it generate 12 clusters to pull out nuanced weather patterns.\n\n\nkmeans = KMeans(n_clusters=12)\nmodel = kmeans.fit(X)\nprint(\"model\\n\", model)\ncenters = model.cluster_centers_\ncenters\n\nC:\\Users\\amals\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\nmodel\n KMeans(n_clusters=12)\n\n\narray([[-1.1766631 , -0.8790686 ,  0.44611167,  1.97215373,  0.53816535,\n         1.93363639,  0.91784593],\n       [-0.1645896 ,  0.86439437, -1.31104187, -0.58965325, -1.16673474,\n        -0.60492156, -0.64099935],\n       [-0.69131268,  0.54908212,  0.17791583, -0.58588629,  0.34773746,\n        -0.5992676 , -0.11805412],\n       [ 1.19070339, -0.25445169, -1.15488277,  2.12034018, -1.05328454,\n         2.23705646, -1.13474293],\n       [-0.84377297, -1.19719742,  0.3745597 ,  0.34187084,  0.47340403,\n         0.32988874,  1.36332625],\n       [-0.21407517,  0.62955451,  0.40836328,  0.7361672 ,  0.51643607,\n         0.67406747, -0.14847653],\n       [ 0.05995891, -0.787475  , -1.19662281, -0.57027401, -1.04258893,\n        -0.58473883,  0.87783959],\n       [ 1.3659396 , -0.08042497, -1.20760939, -0.053456  , -1.07628299,\n        -0.03343366, -0.97722815],\n       [ 0.73330625,  0.43251486,  0.28689726, -0.53075968,  0.47401768,\n        -0.53692476, -0.77193636],\n       [ 0.13140762,  0.84390635,  1.41114256, -0.6388028 ,  1.6750861 ,\n        -0.58964232, -0.71461687],\n       [ 0.23396264,  0.32004883,  1.88793525, -0.65184949, -1.55170412,\n        -0.57668218, -0.28325778],\n       [ 0.26128351, -0.99362107,  0.66249275, -0.54678248,  0.85356601,\n        -0.52916452,  1.15543024]])\n\n\nVisualizing and Interpreting the Clusters\n\n#To interpret the clusters, parallel coordinate plots are perfect. They allow visualizing high-dimensional data across multiple variables. We write functions to easily generate plots coloring each cluster's center values. The visualization reveals interesting insights - clusters clearly capture different weather conditions like dry days or warm fronts. We can also spot transitional patterns between weather states.\n\n\ndef pd_centers(featuresUsed, centers):\n    colNames = list(featuresUsed)\n    colNames.append('prediction')\n\n    # Zip with a column called 'prediction' (index)\n    Z = [np.append(A, index) for index, A in enumerate(centers)]\n\n    # Convert to pandas data frame for plotting\n    P = pd.DataFrame(Z, columns=colNames)\n    P['prediction'] = P['prediction'].astype(int)\n    return P\n\n\ndef parallel_plot(data):\n    my_colors = list(islice(cycle(['b', 'r', 'g', 'y', 'k']), None, len(data)))\n    plt.figure(figsize=(15,8)).gca().axes.set_ylim([-3,+3])\n    parallel_coordinates(data, 'prediction', color = my_colors, marker='o')\n\n\n#Dry Days\nP = pd_centers(features, centers)\nP\nparallel_plot(P[P['relative_humidity'] &lt; -0.5])\n\n\n\n\n\n#Warm Days\nparallel_plot(P[P['air_temp'] &gt; 0.5])\n\n\n\n\n\n#Cool Days\nparallel_plot(P[(P['relative_humidity'] &gt; 0.5) & (P['air_temp'] &lt; 0.5)])\n\n\n\n\nConclusion\n\n#This analysis enhances our understanding of local weather dynamics. Granular clustering uncovers patterns hidden in the raw data. As weather data grows ever more abundant, machine learning techniques will continue unlocking its valuable secrets."
  }
]